{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.nn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABLSTM(object):\n",
    "\t\"\"\"docstring for ABLSTM\"\"\"\n",
    "\tdef __init__(self, config):\n",
    "\t\t#super(ABLSTM, self).__init__()\n",
    "\t\tself.time_step = config[\"time_step\"]\n",
    "\t\tself.hidden_size = config[\"hidden_size\"]\n",
    "\t\t# self.vocab_size = config[\"vocab_size\"]\n",
    "\t\t# self.embedding_size = config[\"embedding_size\"]\n",
    "\t\tself.n_class = config[\"n_class\"]\n",
    "\t\tself.learning_rate = config[\"learning_rate\"]\n",
    "\n",
    "\t\t# placeholser\n",
    "\t\tself.x = tf.compat.v1.placeholder(tf.float32, [None, self.time_step, self.hidden_size])\n",
    "\t\tself.label = tf.compat.v1.placeholder(tf.int32, [None, self.n_class])\n",
    "\t\tself.keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "\t\t\n",
    "\tdef build_graph(self):\n",
    "\t\tprint(\"building graph\")\n",
    "        \n",
    "\t\trnn_outputs, _ = bi_rnn(BasicLSTMCell(self.hidden_size), BasicLSTMCell(self.hidden_size), inputs=self.x, dtype=tf.float32)\n",
    "\n",
    "\t\tfw_output, bw_output = rnn_outputs\n",
    "\n",
    "\t\tH = fw_output + bw_output  # (batch_size, time_step, hidden_size)\n",
    "\t\t\n",
    "\t\t# attention\n",
    "\t\t# att_size = H.shape[2].value\n",
    "\t\tW = tf.Variable(tf.random_normal([self.hidden_size], stddev=0.1))\n",
    "\t\tM = tf.tanh(H)  # (batch_size, time_step, hidden_size)\n",
    "\n",
    "\t\tself.alpha = tf.nn.softmax(tf.tensordot(M, W, axes=1))\n",
    "\t\tr = tf.reduce_sum(H*tf.expand_dims(self.alpha, -1), 1)\n",
    "\t\th_star = tf.tanh(r)  #(batch, hidden_size)\n",
    "\n",
    "\t\t# Dropout layer\n",
    "\t\th_drop = tf.nn.dropout(h_star, self.keep_prob)\n",
    "\n",
    "\t\t# Fully connected layer\n",
    "\t\t# FC_W = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n",
    "  #       FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n",
    "  #       y_hat = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
    "\t\ty_hat = tf.layers.dense(h_drop, self.n_class, kernel_initializer=None)\n",
    "\t\tself.prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "\t\tself.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.label, logits=y_hat)\n",
    "\n",
    "        # optimization\n",
    "# \t\tloss_to_minimize = self.loss\n",
    "# \t\ttvars = tf.trainable_variables()\n",
    "# \t\tgradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "# \t\tgrads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
    "\n",
    "# \t\tself.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "# \t\tself.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "# \t\tself.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step, name='train_step')\n",
    "# \t\tprint(\"graph built successfully!\")\n",
    "\t\tself.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\t\tprint(\"graph built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_X, data_Y, batch_size):\n",
    "    \"\"\"Generator to yield batches\"\"\"\n",
    "    # Shuffle data first.\n",
    "    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n",
    "    # print(\"before shuffle: \", data_Y[:10])\n",
    "    # print(data_X.shape[0])\n",
    "    # perm = np.random.permutation(data_X.shape[0])\n",
    "    # data_X = data_X[perm]\n",
    "    # shuffled_Y = data_Y[perm]\n",
    "    # print(\"after shuffle: \", shuffled_Y[:10])\n",
    "    for idx in range(data_X.shape[0] // batch_size):\n",
    "        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n",
    "        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n",
    "        yield idx, x_batch, y_batch\n",
    "    del shuffled_X, shuffled_Y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_train_step(model, sess, batch):\n",
    "#     feed_dict = {model.x: batch[0], model.label: batch[1], model.keep_prob: 0.5}\n",
    "#     to_return = {\n",
    "#         'train_op': model.train_op,\n",
    "#         'loss': model.loss,\n",
    "#         'global_step': model.global_step,\n",
    "#     }\n",
    "#     return sess.run(to_return, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_weight(model, sess, batch):\n",
    "    feed_dict = {model.x: batch[0], model.label: batch[1], model.keep_prob: 0.5}\n",
    "    return sess.run(model.alpha, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_step(model, sess, batch):\n",
    "    feed_dict = {model.x: batch[0], model.label: batch[1], model.keep_prob: 1.0}\n",
    "    prediction = sess.run(model.prediction, feed_dict)\n",
    "#     acc = tf.metrics.accuracy(labels=tf.argmax(batch[1], 1), predictions=prediction)\n",
    "#     print(\"prediction: \", prediction)\n",
    "#     print(\"The labels: \", batch[1])\n",
    "#     labels = sess.run(tf.argmax(batch[1], 1))\n",
    "#     print(\"labels:\", labels)\n",
    "#     correct_prediction = tf.equal(prediction, labels)\n",
    "#     print(\"correct_prediction:\", correct_prediction)\n",
    "#     acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print(tf.math.argmax(batch[1], 1).shape)\n",
    "    acc = sess.run(tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch[1], 1)), tf.float32)))\n",
    "#     print(acc.shape)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataimport(path1, path2):\n",
    "    window_size = 1000\n",
    "    threshold = 60\n",
    "    slide_size = 200 #less than window_size!!!\n",
    "    \n",
    "    xx = np.empty([0,window_size,90],float)\n",
    "    yy = np.empty([0,8],float)\n",
    "\n",
    "\t###Input data###\n",
    "\t#data import from csv\n",
    "    input_csv_files = sorted(glob.glob(path1))\n",
    "    for f in input_csv_files:\n",
    "        print(\"input_file_name=\",f)\n",
    "        data = [[ float(elm) for elm in v] for v in csv.reader(open(f, \"r\"))]\n",
    "        tmp1 = np.array(data)\n",
    "        x2 =np.empty([0,window_size,90],float)\n",
    "# \t\tprint(tmp1.shape)\n",
    "\n",
    "\t\t#data import by slide window\n",
    "        k = 0\n",
    "        while k <= (len(tmp1) + 1 - 2 * window_size):\n",
    "            x = np.dstack(np.array(tmp1[k:k+window_size, 1:91]).T)\n",
    "# \t\t\tprint(x.shape)\n",
    "            x2 = np.concatenate((x2, x),axis=0)\n",
    "# \t\t\tprint(x2.shape)\n",
    "            k += slide_size\n",
    "\n",
    "        xx = np.concatenate((xx,x2),axis=0)\n",
    "# \t\tprint(xx.shape)\n",
    "# \txx = xx.reshape(len(xx),-1)\n",
    "# \tprint(xx.shape)\n",
    "\n",
    "\n",
    "\t###Annotation data###\n",
    "\t#data import from csv\n",
    "    annotation_csv_files = sorted(glob.glob(path2))\n",
    "    for ff in annotation_csv_files:\n",
    "        print(\"annotation_file_name=\",ff)\n",
    "        ano_data = [[ str(elm) for elm in v] for v in csv.reader(open(ff,\"r\"))]\n",
    "        tmp2 = np.array(ano_data)\n",
    "\n",
    "        #data import by slide window\n",
    "        y = np.zeros(((len(tmp2) + 1 - 2 * window_size)//slide_size+1,8))\n",
    "        k = 0\n",
    "        while k <= (len(tmp2) + 1 - 2 * window_size):\n",
    "            y_pre = np.stack(np.array(tmp2[k:k+window_size]))\n",
    "            bed = 0\n",
    "            fall = 0\n",
    "            walk = 0\n",
    "            pickup = 0\n",
    "            run = 0\n",
    "            sitdown = 0\n",
    "            standup = 0\n",
    "            noactivity = 0\n",
    "            for j in range(window_size):\n",
    "                if y_pre[j] == \"bed\":\n",
    "                    bed += 1\n",
    "                elif y_pre[j] == \"fall\":\n",
    "                    fall += 1\n",
    "                elif y_pre[j] == \"walk\":\n",
    "                    walk += 1\n",
    "                elif y_pre[j] == \"pickup\":\n",
    "                    pickup += 1\n",
    "                elif y_pre[j] == \"run\":\n",
    "                    run += 1\n",
    "                elif y_pre[j] == \"sitdown\":\n",
    "                    sitdown += 1\n",
    "                elif y_pre[j] == \"standup\":\n",
    "                    standup += 1\n",
    "                else:\n",
    "                    noactivity += 1\n",
    "\n",
    "            if bed > window_size * threshold / 100:\n",
    "                y[int(k/slide_size),:] = np.array([0,1,0,0,0,0,0,0])\n",
    "            elif fall > window_size * threshold / 100:\n",
    "                y[int(k/slide_size),:] = np.array([0,0,1,0,0,0,0,0])\n",
    "            elif walk > window_size * threshold / 100:\n",
    "                y[int(k/slide_size),:] = np.array([0,0,0,1,0,0,0,0])\n",
    "            elif pickup > window_size * threshold / 100:\n",
    "                y[int(k/slide_size),:] = np.array([0,0,0,0,1,0,0,0])\n",
    "            elif run > window_size * threshold / 100:\n",
    "                y[int(k/slide_size),:] = np.array([0,0,0,0,0,1,0,0])\n",
    "            elif sitdown > window_size * threshold / 100:\n",
    "                y[int(k/slide_size),:] = np.array([0,0,0,0,0,0,1,0])\n",
    "            elif standup > window_size * threshold / 100:\n",
    "                y[int(k/slide_size),:] = np.array([0,0,0,0,0,0,0,1])\n",
    "            else:\n",
    "                y[int(k/slide_size),:] = np.array([2,0,0,0,0,0,0,0])\n",
    "            k += slide_size\n",
    "\n",
    "        yy = np.concatenate((yy, y),axis=0)\n",
    "    print(xx.shape,yy.shape)\n",
    "    return (xx, yy)\n",
    "\n",
    "def split_dataset(x,y,dev_ratio,test_ratio):\n",
    "    x_size = len(x)\n",
    "    train_dev_size = int(x_size * (1-test_ratio))\n",
    "    x_train_dev = x[:train_dev_size]\n",
    "    x_test = x[train_dev_size:]\n",
    "    y_train_dev = y[:train_dev_size]\n",
    "    y_test = y[train_dev_size:]\n",
    "\n",
    "    train_size = int(x_size * (1-dev_ratio-test_ratio))\n",
    "# \tprint(train_size)\n",
    "    x_train = x_train_dev[:train_size]\n",
    "# \tprint(x_train.shape)\n",
    "    x_dev = x_train_dev[train_size:]\n",
    "    y_train = y_train_dev[:train_size]\n",
    "# \tprint(y_train.shape)\n",
    "    y_dev = y_train_dev[train_size:]\n",
    "\n",
    "    return x_train, x_dev, x_test, y_train, y_dev, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_file_name= /home/yan/work/Att_bi_lstm/data_test/input_bed_170308_1405_01.csv\n",
      "annotation_file_name= /home/yan/work/Att_bi_lstm/data_test/annotation_bed_170308_1405_01.csv\n",
      "(90, 1000, 90) (90, 8)\n",
      "(0, 1000, 90) (0, 8)\n",
      "(0, 1000, 90) (0, 8)\n",
      "(0, 1000, 90) (0, 8)\n",
      "(0, 1000, 90) (0, 8)\n",
      "input_file_name= /home/yan/work/Att_bi_lstm/data_test/input_fall_170310_1142_08.csv\n",
      "annotation_file_name= /home/yan/work/Att_bi_lstm/data_test/annotation_fall_170310_1142_08.csv\n",
      "(90, 1000, 90) (90, 8)\n",
      "input_file_name= /home/yan/work/Att_bi_lstm/data_test/input_pickup_170309_1204_01.csv\n",
      "annotation_file_name= /home/yan/work/Att_bi_lstm/data_test/annotation_pickup_170309_1204_01.csv\n",
      "(90, 1000, 90) (90, 8)\n",
      "building graph\n",
      "WARNING:tensorflow:From <ipython-input-2-48281719f566>:20: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-2-48281719f566>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f088d3f5c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f088d3f5c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f088d3f5c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f088d3f5c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0886504550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0886504550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0886504550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0886504550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-2-48281719f566>:36: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-2-48281719f566>:42: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f087d48b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f087d48b590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f087d48b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f087d48b590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /home/yan/anaconda3/envs/tf_gpu_env/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "graph built successfully!\n",
      "Epoch 1 start!\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d4fcb0a695b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mdev_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mcou\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train step=%d, train loss=%.3f, validation accuracy: %.3f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Epoch time:  %.3f s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # load data\n",
    "    xx = np.empty([0,1000,90],float)\n",
    "    yy = np.empty([0,8],float)\n",
    "    for i, label in enumerate ([\"bed\", \"walk\", \"run\", \"sitdown\", \"standup\", \"fall\", \"pickup\"]):\n",
    "#         print(label,\":\")\n",
    "        filepath1 = \"/home/yan/work/Att_bi_lstm/data_test/input_\" + str(label) + \"*.csv\"\n",
    "        filepath2 = \"/home/yan/work/Att_bi_lstm/data_test/annotation_\" + str(label) + \"*.csv\"\n",
    "        x, y = dataimport(filepath1, filepath2)\n",
    "#         print(\"x:\", x.shape, \"y:\", y.shape)\n",
    "        xx = np.concatenate((xx, x),axis=0)\n",
    "        yy = np.concatenate((yy, y),axis=0)\n",
    "#         print(xx.shape, yy.shape)\n",
    "    xx, yy = shuffle(xx, yy)\n",
    "\n",
    "    x_train, x_dev, x_test, y_train, y_dev, y_test = split_dataset(xx,yy,dev_ratio=0.1,test_ratio=0.1)\n",
    "#     print(\"x_train:\", x_train.shape, \"x_dev:\", x_dev.shape, \"x_test:\", x_test.shape, \"y_train:\", y_train.shape, \"y_test:\", y_test.shape, \"y_dev:\", y_dev.shape)\n",
    "    del xx, yy\n",
    "    gc.collect()\n",
    "    \n",
    "    config = {\n",
    "        \"time_step\": 1000,\n",
    "        \"hidden_size\": 90,\n",
    "        # \"vocab_size\": vocab_size,\n",
    "        # \"embedding_size\": 128,\n",
    "        \"n_class\": 8,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 128,\n",
    "        \"train_epoch\": 3\n",
    "    }\n",
    "\n",
    "    classifier = ABLSTM(config)\n",
    "    classifier.build_graph()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    start = time.time()\n",
    "\n",
    "    for e in range(config[\"train_epoch\"]):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        print(\"Epoch %d start!\" %(e+1))\n",
    "        for c, x_batch, y_batch in fill_feed_dict(x_train, y_train, config[\"batch_size\"]):\n",
    "#             return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n",
    "#             attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n",
    "            train_loss, train_op = sess.run([classifier.loss, classifier.train_op], feed_dict={classifier.x: x_batch, classifier.label: y_batch, classifier.keep_prob: 0.5})\n",
    "            if c%10==0:\n",
    "                cou = 0\n",
    "                dev_acc = 0\n",
    "                for d, x_batch, y_batch in fill_feed_dict(x_dev, y_dev, config[\"batch_size\"]):\n",
    "                    accuracy = run_eval_step(classifier, sess, (x_batch, y_batch))\n",
    "                    dev_acc += accuracy\n",
    "                    cou += 1\n",
    "                print(\"Train step=%d, train loss=%.3f, validation accuracy: %.3f \" % (c, train_loss, dev_acc/cou))\n",
    "        t1 = time.time()\n",
    "        print(\"Train Epoch time:  %.3f s\" % (t1 - t0))\n",
    "#         dev_acc = sess.run(run_eval_step(classifier, sess, (x_dev, y_dev)))\n",
    "#         print(\"validation accuracy: %.3f \" % dev_acc)\n",
    "# #         print(dev_acc.shape)\n",
    "# #         print(\"validation accuracy: \", dev_acc)\n",
    "\n",
    "#     print(\"Training finished, time consumed : \", time.time() - start, \" s\")\n",
    "    print(\"Start evaluating:  \\n\")\n",
    "    cnt = 0\n",
    "    test_acc = 0\n",
    "    for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[\"batch_size\"]):\n",
    "        acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n",
    "        test_acc += acc\n",
    "        cnt += 1\n",
    "\n",
    "    print(\"Test accuracy : %f %%\" % (test_acc / cnt * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
